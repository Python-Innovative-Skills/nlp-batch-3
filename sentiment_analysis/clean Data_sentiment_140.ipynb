{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef562fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, header=None, encoding='latin-1', names=['target','id','date','meta','user','text'])\n",
    "    df['sentiment'] = df['target'].apply(lambda x: 'positive' if x==4 else 'negative')\n",
    "    df.drop(columns=['id','date','meta','user'], inplace=True)\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce4e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target                                               text sentiment\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  negative\n",
      "1       0  is upset that he can't update his Facebook by ...  negative\n",
      "2       0  @Kenichan I dived many times for the ball. Man...  negative\n",
      "3       0    my whole body feels itchy and like its on fire   negative\n",
      "4       0  @nationwideclass no, it's not behaving at all....  negative\n"
     ]
    }
   ],
   "source": [
    "file_path ='sentiment140.csv'\n",
    "\n",
    "df = load_data(file_path)\n",
    "\n",
    "df_subset = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd02e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ed55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "standard_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Print the standard stopwords\n",
    "print(\"Standard Stopwords:\")\n",
    "print(sorted(standard_stopwords))  # Sorted for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d893b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prep_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    print(\"lowercase: \",text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(\"Remove Puncuation: \",text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text) \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"Remove stopwords: \",text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9966191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase:  @switchfoot http://twitpic.com/2y1zl - awww, that's a bummer.  you shoulda got david carr of third day to do it. ;d\n",
      "Remove Puncuation:  switchfoot httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d\n",
      "Remove stopwords:  switchfoot httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d\n",
      "lowercase:  is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!\n",
      "Remove Puncuation:  is upset that he cant update his facebook by texting it and might cry as a result  school today also blah\n",
      "Remove stopwords:  is upset that he cant update his facebook by texting it and might cry as a result  school today also blah\n",
      "lowercase:  @kenichan i dived many times for the ball. managed to save 50%  the rest go out of bounds\n",
      "Remove Puncuation:  kenichan i dived many times for the ball managed to save 50  the rest go out of bounds\n",
      "Remove stopwords:  kenichan i dived many times for the ball managed to save 50  the rest go out of bounds\n",
      "lowercase:  my whole body feels itchy and like its on fire \n",
      "Remove Puncuation:  my whole body feels itchy and like its on fire \n",
      "Remove stopwords:  my whole body feels itchy and like its on fire \n",
      "lowercase:  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there. \n",
      "Remove Puncuation:  nationwideclass no its not behaving at all im mad why am i here because i cant see you all over there \n",
      "Remove stopwords:  nationwideclass no its not behaving at all im mad why am i here because i cant see you all over there \n",
      "lowercase:  @kwesidei not the whole crew \n",
      "Remove Puncuation:  kwesidei not the whole crew \n",
      "Remove stopwords:  kwesidei not the whole crew \n",
      "lowercase:  need a hug \n",
      "Remove Puncuation:  need a hug \n",
      "Remove stopwords:  need a hug \n",
      "lowercase:  @loltrish hey  long time no see! yes.. rains a bit ,only a bit  lol , i'm fine thanks , how's you ?\n",
      "Remove Puncuation:  loltrish hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you \n",
      "Remove stopwords:  loltrish hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you \n",
      "lowercase:  @tatiana_k nope they didn't have it \n",
      "Remove Puncuation:  tatianak nope they didnt have it \n",
      "Remove stopwords:  tatianak nope they didnt have it \n",
      "lowercase:  @twittera que me muera ? \n",
      "Remove Puncuation:  twittera que me muera  \n",
      "Remove stopwords:  twittera que me muera  \n",
      "cleaned_text 0    switchfoot httptwitpiccom2y1zl awww thats bumm...\n",
      "1    upset cant update facebook texting might cry r...\n",
      "2    kenichan dived many times ball managed save 50...\n",
      "3                     whole body feels itchy like fire\n",
      "4             nationwideclass behaving im mad cant see\n",
      "5                                  kwesidei whole crew\n",
      "6                                             need hug\n",
      "7    loltrish hey long time see yes rains bit bit l...\n",
      "8                                  tatianak nope didnt\n",
      "9                                   twittera que muera\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp/ipykernel_8696/513100268.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['cleaned_text'] = df_subset['text'].apply(clean_prep_text)\n"
     ]
    }
   ],
   "source": [
    "df_subset['cleaned_text'] = df_subset['text'].apply(clean_prep_text)\n",
    "\n",
    "print(\"cleaned_text\",df_subset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e497da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 7, y_train shape: 7\n",
      "X_test shape: 3, y_test shape: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xs = df_subset['cleaned_text'].tolist()  # List of input texts\n",
    "ys = df_subset['target'].tolist()          # List of labels\n",
    "\n",
    "# Step 3: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.25, random_state=0)\n",
    "\n",
    "# Optionally, print the shapes of the resulting datasets\n",
    "print(f'X_train shape: {len(X_train)}, y_train shape: {len(y_train)}')\n",
    "print(f'X_test shape: {len(X_test)}, y_test shape: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0fdd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nationwideclass behaving im mad cant see', 'switchfoot httptwitpiccom2y1zl awww thats bummer shoulda got david carr third day', 'whole body feels itchy like fire', 'upset cant update facebook texting might cry result school today also blah', 'loltrish hey long time see yes rains bit bit lol im fine thanks hows', 'tatianak nope didnt', 'kwesidei whole crew'] [0, 0, 0, 0, 0, 0, 0]\n",
      "['twittera que muera', 'upset cant update facebook texting might cry result school today also blah', 'need hug', 'loltrish hey long time see yes rains bit bit lol im fine thanks hows', 'whole body feels itchy like fire', 'switchfoot httptwitpiccom2y1zl awww thats bummer shoulda got david carr third day', 'kwesidei whole crew'] [0, 0, 0, 0, 0, 0, 0]\n",
      "['switchfoot httptwitpiccom2y1zl awww thats bummer shoulda got david carr third day', 'loltrish hey long time see yes rains bit bit lol im fine thanks hows', 'kenichan dived many times ball managed save 50 rest go bounds', 'twittera que muera', 'nationwideclass behaving im mad cant see', 'whole body feels itchy like fire', 'need hug'] [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xs = df_subset['cleaned_text'].tolist()  # List of input texts\n",
    "ys = df_subset['target'].tolist()          # List of labels\n",
    "\n",
    "# Step 3: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.25, random_state=1)\n",
    "\n",
    "print(X_train, y_train)\n",
    "# Step 3: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train, y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807612a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c76ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
